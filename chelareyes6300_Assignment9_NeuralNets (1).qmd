---
title: "Assignment 9: Neural Nets"
author: "Chela Reyes"
date: "[Date]"
format: html
editor: source
self-contained: true
toc: true
toc-expand: true
toc-depth: 3
---

```{r setup, include=FALSE}
#knitr::opts_knit$set(root.dir = '/Users/diana/Babson MSBA/Machine learning')
```

# Context

Let’s take a look at the data located in UniversalBank.csv. Each row represents a customer at small but rapidly growing bank. The columns measure all sorts of customer characteristics, ranging from their demographic information (e.g., Age, Family) to whether they currently have various accounts open with the bank (e.g., Securities Account, CD Account). For a complete description of the fields, consult the data page on Canvas.

The bank is aggressively trying to convert customers from depositors into borrowers through its personal loan program. The column **Personal Loan** shows whether each customer responded to a direct marketing campaign related to this program. The marketing team is now trying to understand what types of customers responds to new personal loans marketing. If they can establish reasonably strong predictive power, they will deploy the model more widely across their customer base to identify promising leads and a more nuanced target market.


# Task 1: Predicting Mortgage Amount

## Question 1

Do the following:

* Import the data as "bank"

* Remove any predictor that might be inappropriate for this activity.

* Set the target to a factor.

```{r}
bank <- read.csv('UniversalBank.csv')
bank$ID <- NULL
bank$Personal.Loan = as.factor(bank$Personal.Loan)
```

## Question 2

* Given the fact that most activation functions are nonlinear, standardizing is an important step towards making sure no one variable swamps all of the others.

Standardize all of the numeric inputs for this data set (e.g., using preProcess from the caret package). 

Note: During class, we normalized our variable. This time, we want to standardize! We have done this in the past!

To verify that this process has gone well, enter the maximum value of the standardized version of the Income variable below. Round your answer to two decimal places.

Answer: 3.26

```{r}
library(nnet)
library(caret)
library(NeuralNetTools)
source("BabsonAnalytics.R")
standar = preProcess(bank, method =c ("center", "scale"))
bank = predict(standar, bank)
max(bank$Income)


```

## Question 3

Write a short sentence putting describing what the quantity you found in the previous question (Q3) means.

Answer: the max standarized income value is 3.26, this means that income can deviate 3.26 from the mean 


## Question 4

* Set a seed of 28 and partition a training set with 70% of the data.

```{r}
set.seed(28)
N = nrow(bank)
trainingSize = round(N*0.7)
trainingCases = sample(N, trainingSize)
train = bank[trainingCases,]
test = bank[-trainingCases,]
```





## Question 5

Build a neural net model with a hidden layer of size 4, and plot it using the plotnet function from the NeuralNetTools package. Notice how bad this default plotting is. Really, really subpar. Now plot with the fix (originally posted here: https://github.com/fawda123/NeuralNetTools/issues/20).

par(mar = numeric(4))
plotnet(model,pad_x = .5)

What was the difference between the original and the fixed versions?

Answer: 
The original plotnet visualization was cluttered, with overlapping connections and poorly aligned labels, making it hard to interpret. The fixed version using pad_x = 0.5 improved the spacing between nodes, reduced overlap, and aligned labels more clearly. This resulted in a much cleaner and more readable visualization of the neural network.

```{r}
model = nnet(Personal.Loan ~ ., data=train, size = 4)
plotnet(model)
par(mar = numeric(4))
plotnet(model,pad_x = .5)
```


## Question 6

What can we learn about what types of customers accept personal loan offers by visualizing examining the neural net you've created? (i.e., what did we say in class from looking at this visual?)

Answer: The neural net visualization shows which features, like income, family size, or education, are most important for predicting if a customer accepts a personal loan. Stronger connections (thicker lines) mean those features have a bigger impact. For example, higher income or more family members might make someone more likely to accept a loan. This helps businesses focus on the most relevant customer groups


## Question 7

Take a look at your confusion matrix. Also, calculate the error rate and benchmark error rate. How many predictions did you get correct from the model? What is the error rate associated with your model? Is this a useful model?

Answer: 1459 prediction where right, the error rate of the model is 0.0273, lower than the benchmark error rate which is 0.089, we can say that  the model is useful.

```{r}
pred = predict(model, test, type="class")

table(pred,test$Personal.Loan)
error_rate = sum(pred != test$Personal.Loan)/nrow(test)

#calculate benchmark error rate
error_bench = benchmarkErrorRate(train$Personal.Loan, test$Personal.Loan)

(117+17) /(1342+17+24+117)
```



## Question 8

One could make the argument that we care very much about predicting the True observations correctly in this model. What is sensitivity of this model? Round your answer to two decimal places, e.g., 0.12

Also, how does this rate - essentially an accuracy rate - compare with the error rate you obtained previously? What is the issue with this if predicting True's accurately was really important to you in this context?

Answer: the sensitivity of the model is 87.31%, this means 87% trues where predicted correctly with the model.

```{r}
sensitivity <- sum(pred == 1 & test$Personal.Loan ==1)/sum(test$Personal.Loan == 1)
sensitivity
```


## Question 9

What happens if you don't standardize? Turn off standardization and re-run your neural network(i.e., Just use hashtags to comment out a couple lines in your code and re-run everything). You will likely get an error if you try to compute the error rate, so just look at the misclassification table instead. What do you see? What you do think happened here?

After you answer this question, just make sure to un-comment your lines of code before knitting.

Answer: The model is good at finding false positives but not very good at predicting true positives.
The model is good at finding false positives but not very good at predicting true positives. This might happen because the data isn’t standardized, so variables with big values get more importance, making smaller variables less important.


